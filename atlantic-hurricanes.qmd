---
title: "Hurricanes in the Atlantic"
subtitle: "An assessment of the factors driving the costs of hurricane damage."
author: "Carmen Hoyt"
date: last-modified
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  messages: false
format:
  html:
      toc: true
      code-fold: true
---

# About

Growing up on the Gulf Coast of Florida, my family and I were no stranger to hurricanes. We have lived in the same house for over 20 years, but recent hurricane seasons[^1] have brought an increase in flooding and severe damage. For example, Hurricane Ian (2022) stands out as having been the worst storm to hit our neighborhood during my lifetime, bringing an unprecedented 7 feet of storm surge into our home. 

[^1]: Hurricane season runs June 1 to November 30, but since 2021, the [National Hurricane Center (NHC) has considered moving the start date to May 15](https://www.accuweather.com/en/hurricane/nhc-considering-change-to-start-date-of-hurricane-season/1168271) to encompass increasing early-season activity.

Climate change is influencing natural disasters across the board, and hurricanes are no exception. While the frequency of storms is not projected to change, they are predicted to become more intense[^2]. Hurricane "intensity" is measured in categories on the Saffir-Simpson scale: 1 through 5 based on wind speed[^3]. Storms assigned a category 3 or higher are considered "major hurricanes" and are expected to bring catastrophic damage (and thus more significant damage costs). This leads to my question:

**As storms increase in intensity, are damage costs also being driven by time?**

[^2]: Read more about how hurricanes are expected to evolve with climate change [here](https://science.nasa.gov/earth/climate-change/a-force-of-nature-hurricanes-in-a-changing-climate/).

[^3]: Read more about the Saffir-Simpson scale [here](https://www.weather.gov/mfl/saffirsimpson#:~:text=The%20Saffir%2DSimpson%20Hurricane%20Wind,loss%20of%20life%20and%20damage.).

# Data

I will be using Kaggle's [North American Hurricanes from 2000](https://www.kaggle.com/datasets/middlehigh/north-american-hurricanes-from-2000) dataset to develop a model of the factors driving `damage costs`. Of these factors:

**I predict that time has an effect on damage costs (increasing over time).**

- H0: Time has no effect on damage costs.
- HA: Time has an effect on damage costs.

## Explore and Clean Data

### Import packages
```{r}
#| code-summary: Expand Code
# Load required packages
library(tidyverse)
library(here)
library(janitor)
library(patchwork)
library(kableExtra)
library(webshot2)
```

### Import data
```{r}
#| code-summary: Expand Code
# Remove scientific notation
options(scipen=999)

# Import hurricane data
hurricane_data <- read_csv(here("data", "Hurricane Data.csv")) %>%
  clean_names()
```

### Inspect NA values for the response variable

How are the NA values for `damage costs` distributed across hurricane categories?
```{r}
#| code-summary: Expand Code
# Total storms by category
total_storms <- hurricane_data %>%
  group_by(category) %>%
  summarise(count = n()) %>%
  rename(Category = category,
         Count = count)

print(paste("There are", sum(is.na(hurricane_data$damage_usd)), "NA values associated with damage cost."))

# Check NA values for damage (by category)
na_storms <- hurricane_data %>%
  filter(is.na(damage_usd))%>%
  group_by(category) %>%
  summarise(count = n()) %>%
  rename(Category = category,
         Count = count)

na_table <- left_join(total_storms, na_storms, by = "Category") %>%
  rename("Total Storms" = Count.x,
         "NA Damage" = Count.y) %>%
  arrange(factor(Category, levels = c('TS', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5'))) %>%
  kbl() %>%
  kable_styling()

na_table
```
Since NA values are somewhat proportionally distributed, it appears that removing them will not have an impact on the analysis. Additionally, I will scale `damage costs` to be represented by "Millions of USD", saved as `damage_mil`.

```{r}
#| code-summary: Expand Code
# Remove rows with NA values for damage costs
hurricane_data_cleaned <- hurricane_data[!is.na(hurricane_data$damage_usd),]

# Find total number of areas affected
hurricane_data_cleaned <- hurricane_data_cleaned %>%
  separate_longer_delim(affected_areas, ",") %>%
  group_by(year, name, category, rain_inch, highest_wind_speed, damage_usd, fatalities) %>%
  summarise(total_areas = n()) %>%
  # Scale down damage costs
  mutate(damage_mil = damage_usd/1000000,
         time = year - 2000)
```

# Visualize
```{r}
#| code-summary: Expand Code
# Cost of damage by category
ggplot(hurricane_data_cleaned, aes(x = factor(category, levels = c("TS", "Category 1", "Category 2", "Category 3", "Category 4", "Category 5"), labels = c("TS", "1", "2", "3", "4", "5")), y = damage_mil)) +
  geom_boxplot() +
  labs(x = "Category",
       y = "Damage (Millions of USD)",
       title = "Cost of Storm Damage by Category 2000-2023") +
  theme_minimal()

ggsave(here("graphs", "damage_category_boxplot.jpg"))

mean_median <- hurricane_data_cleaned %>%
  group_by(category) %>%
  summarise(median = median(damage_usd/1000000),
            mean = mean(damage_usd/1000000)) %>%
  arrange(factor(category, levels = c('TS', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5'))) %>%
  rename(Category = category,
         "Median Damage Costs" = median,
         "Mean Damage Costs" = mean) %>%
  kbl() %>%
  kable_styling()

mean_median
```

The `damage costs` data appear to be right-skewed across most hurricane categories (with the exception of category 2). Mean damage costs are higher than median damage costs due to more weight/outliers on the right tail (high end) of the observations. Let's take a closer look at the `damage costs` data. We may want to log-transform these data to make them more "normal". 



```{r}
#| code-summary: Expand Code
# Histogram of damage costs
hist_original <- ggplot(hurricane_data_cleaned, aes(damage_mil)) +
  geom_histogram(bins = round(sqrt(length(hurricane_data_cleaned$damage_mil))), # set number of bins
                 fill = "cornflowerblue", 
                 color = "black") +
  labs(x= "Damage Costs (Millions of USD)",
       y = "Count",
       #title = "Distribution of Damage Cost Data"
       ) +
  theme_minimal()

# Histogram of log damage costs
hist_log <- ggplot(hurricane_data_cleaned, aes(log(damage_mil))) +
  geom_histogram(bins = round(sqrt(length(log(hurricane_data_cleaned$damage_mil)))), # set number of bins
                 fill = "cornflowerblue", 
                 color = "black") +
  labs(x= "Log Damage Costs (Millions of USD)",
       y = "Count",
       #title = "Distribution of Log Damage Cost Data"
       ) +
  theme_minimal()

hist_original + hist_log
```

Using `geom_qq_line()`, we can draw a line indicating where the sample quantiles would lie if the data were normally distributed to assess our log transformation.

```{r}
#| code-summary: Expand Code
qq_original <- ggplot(hurricane_data_cleaned, aes(sample = damage_mil)) +
  geom_qq(color = "cornflowerblue") +
  geom_qq_line() +
  labs(title = "Damage Costs (Millions of USD)") +
  theme_minimal()

qq_log <- ggplot(hurricane_data_cleaned, aes(sample = log(damage_mil))) +
  geom_qq(color = "cornflowerblue") +
  geom_qq_line() +
  labs(title = "Log of Damage Costs (Millions of USD)") +
  theme_minimal()

qq_original / qq_log
```

The log transformed data fit much closer to a normal distribution. 

# Visualize

Since category 5 storms include wind speeds of 157+ mph, it is possible that increasing log(`damage costs`) are associated with an increasing number of category 5 storms. The open-ended category will amass more storms as `max wind speeds` are expected to increase over time.

```{r}
#| code-summary: Expand Code
#| message: false
# Wind speed over time
ggplot(hurricane_data_cleaned, aes(x = year, y = highest_wind_speed)) +
  geom_point() +
  labs(x = "Year",
       y = "Max Wind Speed (mph)",
       title = "Max Wind Speed 2000-2023") +
  geom_hline(yintercept =157,
             linetype = "dashed",
             color = "red") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()
```

You can see how the time period 2000-2014 had 2 category 5 storms while the time period 2015-2023 has 8. 

Since rainfall is also expected to increase with climate change, we might want to include it in our model.

```{r}
#| code-summary: Expand Code
#| message: false
# Rainfall over time
ggplot(hurricane_data_cleaned, aes(x = year, y = rain_inch)) +
  geom_point() +
  labs(x = "Year",
       y = "Rainfall (in)",
       title = "Rainfall 2000-2023") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()
```

The last factor that may be playing a role in log(`damage costs`) is the `number of places affected`, calculated by counting how many "areas" are listed in the `areas_affected` column from the original dataset. As storms increase in intensity, I would expect that they will impact more places over time; however, we see that this may not be the case. We will still explore a potential relationship between `number of places affected` and log(`damage costs`).

```{r}
#| code-summary: Expand Code
#| message: false
# Number of places over time
ggplot(hurricane_data_cleaned, aes(x = year, y = total_areas)) +
  geom_point() +
  labs(x = "Year",
       y = "Number of Places Affected",
       title = "Number of Places Affected 2000-2023") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()
```

`max wind speed`, `rainfall`, `number of places affected`, and `time` are interesting factors that are likely driving log(`damage costs`). Let's visualize their individual relationships with log(`damage costs`).

```{r}
#| code-summary: Expand Code
#| message: false
# Rainfall over time
wind_damage <- ggplot(hurricane_data_cleaned, aes(x = highest_wind_speed, y = log(damage_mil))) +
  geom_point() +
  labs(x = "Max Wind Speed",
       y = "Log of Damage",
       title = "Log of Damage Costs vs. Wind") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

rain_damage <- ggplot(hurricane_data_cleaned, aes(x = rain_inch, y = log(damage_mil))) +
  geom_point() +
  labs(x = "Rainfall (in)",
       y = "Log of Damage",
       title = "Log of Damage Costs vs. Rainfall") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

# Number of places vs. damage costs
areas_damage <- ggplot(hurricane_data_cleaned, aes(x = total_areas, y = log(damage_mil))) +
  geom_point() +
  labs(x = "Number of Places Affected",
       y = "Log of Damage",
       title = "Log of Damage Costs vs. Number of Places Affected") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

# Damage costs over time
time_damage <- ggplot(hurricane_data_cleaned, aes(x = time, y = log(damage_mil))) +
  geom_point() +
  labs(x = "Time (Years since 2000",
       y = "Log of Damage",
       title = "Log of Damage vs. Time") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

wind_damage + rain_damage + areas_damage + time_damage
```

# Analysis

## Multiple Regression Model

We will be testing my hypothesis by normal approximation of the regression coefficient for `time`. In addition to `time`, we must include all relevant predictor variables in our model to reduce the risk of omitted variable bias.

We will use forward selection to assess which of the following predictor variables to include in our model of log(`damage costs`):

- `max wind speed`: highest wind speed achieved by the storm (mph)
- `rainfall`: rain that fell (inches)
- `time`: years since 2000
- `number of palces affected`: a count of the number of places listed in `areas affected`

```{r}
#| code-summary: Expand Code
print(paste("Predict damage from wind:", summary(lm(log(damage_mil)~highest_wind_speed, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from rain:", summary(lm(log(damage_mil)~rain_inch, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predice damage from time:", summary(lm(log(damage_mil)~time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from number of places:", summary(lm(log(damage_mil)~total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
Log(`damage costs`) predictions from `max wind speed` yielded the highest adjusted R2 value. So, we will start with `max wind speed`.

```{r}
#| code-summary: Expand Code
print(paste("Precit damage from wind and rain:", summary(lm(log(damage_mil)~ highest_wind_speed + rain_inch, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind and time:", summary(lm(log(damage_mil)~ highest_wind_speed + time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind and number of places:", summary(lm(log(damage_mil)~ highest_wind_speed + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
Next, we will add `rainfall` since the adjusted R2 increased from 0.276673 to 0.344091.

```{r}
#| code-summary: Expand Code
print(paste("Predict damage from wind, rain, and time:", summary(lm(log(damage_mil)~ highest_wind_speed + rain_inch + time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind, rain, and number of places:", summary(lm(log(damage_mil)~ highest_wind_speed + rain_inch + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
Third, we will add `number of places affected` since the adjusted R2 increased from 0.344091 to 0.376580.

```{r}
#| code-summary: Expand Code
print(paste("Predict damage from wind, rain, number of places, and time:", summary(lm(log(damage_mil)~ highest_wind_speed + rain_inch + time + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
Finally, we will **not** include `time` since it did not increase the adjusted R2 from the previous baseline of 0.376580. 

## Multiple Regression Model
```{r}
#| code-summary: Expand Code
# Create the model
log_damage_model <- lm(log(damage_mil) ~ highest_wind_speed + rain_inch + total_areas, data = hurricane_data_cleaned)
summary(log_damage_model)
```

### Beta 1: Max Wind Speed
```{r}
#| code-summary: Expand Code
# Distribution of Beta1
beta1_estimate <- summary(log_damage_model)$coefficients[2, 1]
beta1_se <- summary(log_damage_model)$coefficients[2, 2]

# Under null hypothesis
tibble(beta1 = seq(-(beta1_estimate + beta1_se),
                   beta1_estimate + beta1_se,
                   length.out = 200),
       density = dnorm(beta1, mean = 0, sd = beta1_se)) %>% 
  # Visualize
  ggplot(aes(beta1, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta1_estimate, color = "firebrick") +
  labs(x = "Beta 1",
       y = "Density") +
  theme_minimal()
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta1 <- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)
print(paste0("The p-value for Beta 1 is: ", pval_beta1, "."))
```

### Beta 2: Rainfall
```{r}
#| code-summary: Expand Code
# Distribution of Beta2
beta2_estimate <- summary(log_damage_model)$coefficients[3, 1]
beta2_se <- summary(log_damage_model)$coefficients[3, 2]

# Visualize
tibble(beta2 = seq(-(beta2_estimate + beta2_se),
                   beta2_estimate + beta2_se,
                   length.out = 200),
       density = dnorm(beta2, mean = 0, sd = beta2_se)) %>% 
  ggplot(aes(beta2, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta2_estimate, color = "firebrick") +
  labs(x = "Beta 2",
       y = "Density") +
  theme_minimal()
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta2 <- 2 * pnorm(-abs(beta2_estimate), mean = 0, sd = beta2_se)
print(paste0("The p-value for Beta 2 is: ", pval_beta2, "."))
```

### Beta 3: Number of Places Affected
```{r}
#| code-summary: Expand Code
# Distribution of Beta2
beta3_estimate <- summary(log_damage_model)$coefficients[4, 1]
beta3_se <- summary(log_damage_model)$coefficients[4, 2]

# Visualize
tibble(beta3 = seq(-(beta3_estimate + beta3_se),
                   beta3_estimate + beta3_se,
                   length.out = 200),
       density = dnorm(beta3, mean = 0, sd = beta3_se)) %>% 
  ggplot(aes(beta3, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta2_estimate, color = "firebrick") +
  labs(x = "Beta 3",
       y = "Density") +
  theme_minimal()
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta3 <- 2 * pnorm(-abs(beta3_estimate), mean = 0, sd = beta3_se)
print(paste0("The p-value for Beta 3 is: ", pval_beta3, "."))
```


## Conclusions

I found that, of my original considerations for predictor variables, `max wind speed`, `rainfall`, and `time` were the best predictors for `damage costs`. The `number of places affected` did not prove to be beneficial to add to the model.

I also found that the p-value for Beta 3 (`time`) was > $alpha = 0.05$ for both models. Thus, we fail to reject the null hypothesis. Remember:

- H0: Time has no effect on storm damage costs.

So, to conclude, we could not rule out that the effect of `time` on `damage costs` could be due to random chance. 

A few factors are not considered in these models:

- cost of inflation

Additional predictor variables that would be of interest:

- storm surge
- water temperature

## Next steps

Given more time, I would have been interested in compiling a better dataset. The dataset used in this analysis isn't the best (no original source or thorough descriptions of variables i.e. rainfall?).


https://www.ncei.noaa.gov/access/billions/


