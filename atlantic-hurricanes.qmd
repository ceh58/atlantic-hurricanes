---
title: "Hurricanes in the Atlantic"
subtitle: "An assessment of the factors driving the costs of hurricane damage."
author: "Carmen Hoyt"
date: last-modified
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  messages: false
format:
  html:
      toc: true
      code-fold: true
---

# About

Growing up on the Gulf Coast of Florida, my family and I were no stranger to hurricanes. We have lived in the same house for over 20 years, but we have seen consistent flooding and severe damage with more recent hurricane seasons[^1]. For example, Hurricane Ian (2022) stands out as having been the worst storm to hit our neighborhood in my lifetime, bringing with it over 7 feet of storm surge into our home. 

[^1]: Hurricane season runs June 1 to November 30, but since 2021, the [National Hurricane Center (NHC) has considered moving the start date to May 15](https://www.accuweather.com/en/hurricane/nhc-considering-change-to-start-date-of-hurricane-season/1168271) to encompass increasing early-season activity.

Hurricanes are assigned categories 1 through 5 on the Saffir-Simpson scale by wind speed[^2]. Storms given a rating of 3 or higher are considered "major hurricanes" and are expected to bring catastrophic damage (and thus more significant damage costs).

[^2]: Read more about the Saffir-Simpson scale [here](https://www.weather.gov/mfl/saffirsimpson#:~:text=The%20Saffir%2DSimpson%20Hurricane%20Wind,loss%20of%20life%20and%20damage.).

# Data

I will be using Kaggle's [North American Hurricanes from 2000](https://www.kaggle.com/datasets/middlehigh/north-american-hurricanes-from-2000) dataset to develop a model of the factors potentially driving storm `damage costs`, including `max wind speed`, `rainfall`, `time`, and the `number of places affected`.

**I predict that time has an effect on storm damage costs (increasing over time).**

- H0: Time has no effect on storm damage costs.
- HA: Time has an effect on storm damage costs.

## Import packages
```{r}
#| code-summary: Expand Code
# Load required packages
library(tidyverse)
library(here)
library(janitor)
library(patchwork)
library(kableExtra)
library(webshot2)
```

## Import data
```{r}
#| code-summary: Expand Code
# Remove scientific notation
options(scipen=999)

# Import hurricane data
hurricane_data <- read_csv(here("data", "Hurricane Data.csv")) %>%
  clean_names()
```

### Inspect NA values for the response variable
How are the NA values for `damage costs` distributed by category?
```{r}
#| code-summary: Expand Code
# Total storms by category
total_storms <- hurricane_data %>%
  group_by(category) %>%
  summarise(count = n()) %>%
  rename(Category = category,
         Count = count)

print(paste("There are", sum(is.na(hurricane_data$damage_usd)), "NA values associated with damage cost."))

# Check NA values for damage (by category)
na_storms <- hurricane_data %>%
  filter(is.na(damage_usd))%>%
  group_by(category) %>%
  summarise(count = n()) %>%
  rename(Category = category,
         Count = count)

na_table <- left_join(total_storms, na_storms, by = "Category") %>%
  rename("Total Storms" = Count.x,
         "NA Damage" = Count.y) %>%
  arrange(factor(Category, levels = c('TS', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5'))) %>%
  kbl() %>%
  kable_styling()

na_table

save_kable(na_table, "na_table.jpg")
```
It appears that removing the NA values will not have an impact on the analysis.

Next, I will scale the `damage costs` in the dataset down to be represented by Millions of USD.
```{r}
#| code-summary: Expand Code
# Remove rows with NA values for damage costs
#complete_df <- na.omit(hurricane_data)
hurricane_data_cleaned <- hurricane_data[!is.na(hurricane_data$damage_usd),]

# Find total number of areas affected
hurricane_data_cleaned <- hurricane_data_cleaned %>%
  separate_longer_delim(affected_areas, ",") %>%
  group_by(year, name, category, rain_inch, highest_wind_speed, damage_usd, fatalities) %>%
  summarise(total_areas = n()) %>%
  # Scale down damage costs
  mutate(damage_mil = damage_usd/1000000,
         time = year - 2000)
```

## Visualize
```{r}
#| code-summary: Expand Code
# Cost of damage by category
ggplot(hurricane_data_cleaned, aes(x = factor(category, levels = c("TS", "Category 1", "Category 2", "Category 3", "Category 4", "Category 5"), labels = c("TS", "1", "2", "3", "4", "5")), y = damage_mil)) +
  geom_boxplot() +
  labs(x = "Category",
       y = "Damage (Millions of USD)",
       title = "Cost of Storm Damage by Category 2000-2023") +
  theme_minimal()

ggsave("damage_category_boxplot.jpg")

mean_median <- hurricane_data_cleaned %>%
  group_by(category) %>%
  summarise(median = median(damage_usd/1000000),
            mean = mean(damage_usd/1000000)) %>%
  arrange(factor(category, levels = c('TS', 'Category 1', 'Category 2', 'Category 3', 'Category 4', 'Category 5'))) %>%
  rename(Category = category,
         "Median Damage Costs" = median,
         "Mean Damage Costs" = mean) %>%
  kbl() %>%
  kable_styling()

mean_median

save_kable(mean_median, "mean_median_table.jpg")
```
The `damage costs` data appear to be right-skewed (the means are higher than the medians due to more weight/outliers on the right tail (high end) of damage cost observations).

A closer look at `damage costs` over the years.
```{r}
#| code-summary: Expand Code
# Damage costs over time
ggplot(hurricane_data_cleaned, aes(x = year, y = damage_mil)) +
  geom_point() +
  labs(x = "Year",
       y = "Damage (Millions of USD)",
       title = "Damage Costs 2000-2023") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

ggsave("damage_over_years.jpg")
```

Let's break that down by category.
```{r}
#| code-summary: Expand Code
# Cost of damage over time
ggplot(hurricane_data_cleaned, aes(x = year, y = damage_mil, color = factor(category, levels = c("TS", "Category 1", "Category 2", "Category 3", "Category 4", "Category 5"), labels = c("TS", "1", "2", "3", "4", "5")))) +
  geom_point() +
  labs(x = "Year",
       y = "Damage (Millions of USD)",
       title = "Damage Costs 2000-2023 by Category",
       color = "Category") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  scale_color_brewer(palette = "Reds") +
  theme_minimal()

ggsave("damage_years_category.jpg")
```

Since category 5 storms include wind speeds of 157+ mph, it is possible that increasing `damage costs` are associated with an increase in `max wind speeds` over time.
```{r}
#| code-summary: Expand Code
# Wind speed over time
ggplot(hurricane_data_cleaned, aes(x = year, y = highest_wind_speed)) +
  geom_point() +
  labs(x = "Year",
       y = "Max Wind Speed (mph)",
       title = "Max Wind Speed 2000-2023") +
  geom_hline(yintercept =157,
             linetype = "dashed",
             color = "red") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

ggsave("wind_years.jpg")
```
You can see 8 category 5 storms (with 157+ winds) between 2015 and 2023, as compared to just 2 between the years 2000-2015.

So far `max wind speed` and `time` are interesting potential factors driving storm `damage costs`. It is likely that there are a few more. Perhaps rainfall has an effect?

```{r}
#| code-summary: Expand Code
# Rainfall over time
ggplot(hurricane_data_cleaned, aes(x = year, y = rain_inch)) +
  geom_point() +
  labs(x = "Year",
       y = "Rainfall (in)",
       title = "Rainfall 2000-2023") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1) +
  theme_minimal()

ggsave("rain_years.jpg")
```
We will explore this more in the next section.

# Analysis

Multiple Linear Regression

**Relevant sample statistic:** regression coefficient

**Predictor variables:**

- `max wind speed`: highest wind speed achieved by the storm (mph)
- `rainfall`: rain that fell (inches)
- `time`: years since 2000
- `number of places affected`: a count of how many areas were listed in the `areas affected` variable

**Response variable:**

- `damage costs`: cost of damage (scaled to millions of USD)

Use forward selection to construct a model for predicting damage costs:
```{r}
print(paste("Predict damage from wind:", summary(lm(damage_mil~highest_wind_speed, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from rain:", summary(lm(damage_mil~rain_inch, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predice damage from time:", summary(lm(damage_mil~time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from number of areas:", summary(lm(damage_mil~total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
So, we will start with `max wind speed`.

```{r}
print(paste("Precit damage from wind and rain:", summary(lm(damage_mil~ highest_wind_speed + rain_inch, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind and time:", summary(lm(damage_mil~ highest_wind_speed + time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind and number of areas:", summary(lm(damage_mil~ highest_wind_speed + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
We will add `rainfall`.

```{r}
print(paste("Predict damage from wind, rain, and time:", summary(lm(damage_mil~ highest_wind_speed + rain_inch + time, data = hurricane_data_cleaned))$adj.r.squared))

print(paste("Predict damage from wind, rain, and number of areas:", summary(lm(damage_mil~ highest_wind_speed + rain_inch + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
We will add `time`.

```{r}
print(paste("Predict damage from wind, rain, time, and number of areas:", summary(lm(damage_mil~ highest_wind_speed + rain_inch + time + total_areas, data = hurricane_data_cleaned))$adj.r.squared))
```
We will not include `number of places` since it did not increase the adjusted R2 from the previous baseline. 

## Plots
```{r}
#| code-summary: Expand Code
wind <- ggplot(hurricane_data_cleaned, aes(x = highest_wind_speed, y = damage_mil)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # geom_abline(intercept = coef(damage_model)[1],
  #             slope = coef(damage_model)[2],
  #             color = "blue") +
  labs(x = "Max Wind Speed (mph)",
       y = "Damage (Millions of USD)",
       title = "Cost by Max Wind Speed") +
  theme_minimal()

rain <- ggplot(hurricane_data_cleaned, aes(x = rain_inch, y = damage_mil)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # geom_abline(intercept = coef(damage_model)[1],
  #             slope = coef(damage_model)[3],
  #             color = "blue") +
  labs(x = "Rainfall (in)",
       y = "Damage (Millions of USD)",
       title = "Cost by Rainfall") +
  theme_minimal()

time <- ggplot(hurricane_data_cleaned, aes(x = time, y = damage_mil)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # geom_abline(intercept = coef(damage_model)[1],
  #             slope = coef(damage_model)[4],
  #             color = "blue") +
  labs(x = "Years since 2000",
       y = "Damage (Millions of USD)",
       title = "Cost by Time") +
  theme_minimal()

# Grid plots
wind / rain / time

ggsave("normal_grid.jpg")
```

## Linear Regression Model

Our model will include:

- `max wind speed`
- `rainfall`
- and `time` 

as predictor variables for storm `damage costs` (in millions of USD).
```{r}
#| code-summary: Expand Code
# Create the model
damage_model <- lm(damage_mil ~ highest_wind_speed + rain_inch + time, data = hurricane_data_cleaned)
summary(damage_model)
```

### 1. Max Wind Speed
```{r}
#| code-summary: Expand Code
# Distribution of Beta1
beta1_estimate <- summary(damage_model)$coefficients[2, 1]
beta1_se <- summary(damage_model)$coefficients[2, 2]

# Under null hypothesis
tibble(beta1 = seq(-(beta1_estimate + beta1_se),
                   beta1_estimate + beta1_se,
                   length.out = 200),
       density = dnorm(beta1, mean = 0, sd = beta1_se)) %>% 
  # Visualize
  ggplot(aes(beta1, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta1_estimate, color = "firebrick") +
  labs(x = "Beta 1",
       y = "Density") +
  theme_minimal()

ggsave("normal_beta1_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta1 <- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)
print(paste0("The p-value for Beta 1 is: ", pval_beta1, "."))
```

### 2. Rainfall
```{r}
#| code-summary: Expand Code
# Distribution of Beta2
beta2_estimate <- summary(damage_model)$coefficients[3, 1]
beta2_se <- summary(damage_model)$coefficients[3, 2]

# Visualize
tibble(beta2 = seq(-(beta2_estimate + beta2_se),
                   beta2_estimate + beta2_se,
                   length.out = 200),
       density = dnorm(beta2, mean = 0, sd = beta2_se)) %>% 
  ggplot(aes(beta2, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta2_estimate, color = "firebrick") +
  labs(x = "Beta 2",
       y = "Density") +
  theme_minimal()

ggsave("normal_beta2_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta2 <- 2 * pnorm(-abs(beta2_estimate), mean = 0, sd = beta2_se)
print(paste0("The p-value for Beta 2 is: ", pval_beta2, "."))
```

### 3. Time
```{r}
#| code-summary: Expand Code
# Distribution of Beta3
beta3_estimate <- summary(damage_model)$coefficients[4, 1]
beta3_se <- summary(damage_model)$coefficients[4, 2]

# Visualize
tibble(beta3 = seq(-(beta3_estimate + beta3_se),
                   beta3_estimate + beta3_se,
                   length.out = 200),
       density = dnorm(beta3, mean = 0, sd = beta3_se)) %>% 
  ggplot(aes(beta3, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta3_estimate, color = "firebrick") +
  labs(x = "Beta 3",
       y = "Density") +
  theme_minimal()

ggsave("normal_beta3_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta3
pval_beta3 <- 2 * pnorm(-abs(beta3_estimate), mean = 0, sd = beta3_se)
print(paste0("The p-value for Beta 3 is: ", pval_beta3, "."))
```
Our p-value for time is greater than $alpha = 0.05$, so we cannot reject the null hypothesis; thus, we cannot rule out that the effect of `time` on storm `damage costs` is due to random chance.

## Log Damage 

Let's take a closer look at the `damage costs` data.
```{r}
#| code-summary: Expand Code
# Histogram of damage costs
ggplot(hurricane_data_cleaned, aes(damage_mil)) +
  geom_histogram(bins = round(sqrt(length(hurricane_data_cleaned$damage_mil))), # set number of bins
                 fill = "cornflowerblue", 
                 color = "black") +
  labs(x= "Damage Costs (Millions of USD)",
       y = "Count",
       title = "Distribution of Damage Cost Data") +
  theme_minimal()

ggsave("hist_damage.jpg")
```
As we noticed before, our damage cost data is right-skewed. Adding a log transformation makes these data more "normal". 

## Plots
Wind:
```{r}
#| code-summary: Expand Code
log_wind <- ggplot(hurricane_data_cleaned, aes(x = highest_wind_speed, y = log(damage_mil))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  #geom_abline(intercept = coef(log_damage_model)[1],
        #      slope = coef(log_damage_model)[2],
         #     color = "blue") +
  labs(x = "Max Wind Speed (mph)",
       y = "Log Damage (Millions of USD)",
       title = "Log Cost by Max Wind Speed") +
  theme_minimal()

log_rain <- ggplot(hurricane_data_cleaned, aes(x = rain_inch, y = log(damage_mil))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  #geom_abline(intercept = coef(damage_model)[1],
   #           slope = coef(damage_model)[3],
    #          color = "blue") +
  labs(x = "Rainfall (in)",
       y = "Log Damage (Millions of USD)",
       title = "Log Cost by Rainfall") +
  theme_minimal()

log_time <- ggplot(hurricane_data_cleaned, aes(x = time, y = log(damage_mil))) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  # geom_abline(intercept = coef(damage_model)[1],
  #             slope = coef(damage_model)[4],
  #             color = "blue") +
  labs(x = "Years Since 2000",
       y = "Log Damage (Millions of USD)",
       title = "Log Cost by Time") +
  theme_minimal()

log_wind / log_rain / log_time
ggsave("log_grid.jpg")
```

New Multiple Linear Regression Model
```{r}
#| code-summary: Expand Code
# Create the model
log_damage_model <- lm(log(damage_mil) ~ highest_wind_speed + rain_inch + time, data = hurricane_data_cleaned)
summary(log_damage_model)
```

```{r}
# Histogram of log damage costs
ggplot(hurricane_data_cleaned, aes(log(damage_mil))) +
  geom_histogram(bins = round(sqrt(length(log(hurricane_data_cleaned$damage_mil)))), # set number of bins
                 fill = "cornflowerblue", 
                 color = "black") +
  labs(x= "Log Damage Costs (Millions of USD)",
       y = "Count",
       title = "Distribution of Log Damage Cost Data") +
  theme_minimal()

ggsave("log_damage.jpg")
```

To see if the log damage model is a better fit, we can use a Scale-Location[^3] plot to observe the residuals.  

[^3]: Adapted from [here](https://library.virginia.edu/data/articles/interpreting-log-transformations-in-a-linear-model).

```{r}
# Original model
plot(damage_model, which = 3) 

# Log model
plot(log_damage_model, which = 3)
```

## Explore factors driving **log** damage costs

### 1. Max Wind Speed
```{r}
#| code-summary: Expand Code
# Distribution of Beta1
beta1_estimate <- exp(summary(log_damage_model)$coefficients[2, 1])
beta1_se <- exp(summary(log_damage_model)$coefficients[2, 2])

# Under null hypothesis
tibble(beta1 = seq(-(beta1_estimate + beta1_se),
                   beta1_estimate + beta1_se,
                   length.out = 200),
       density = dnorm(beta1, mean = 0, sd = beta1_se)) %>% 
  # Visualize
  ggplot(aes(beta1, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta1_estimate, color = "firebrick") +
  labs(x = "Beta 1",
       y = "Density") +
  theme_minimal()

ggsave("log_beta1_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta1 <- 2 * pnorm(-abs(beta1_estimate), mean = 0, sd = beta1_se)
print(paste0("The p-value for beta 1 is: ", pval_beta1, "."))
```

### 2. Rainfall
```{r}
#| code-summary: Expand Code
# Distribution of Beta2
beta2_estimate <- exp(summary(log_damage_model)$coefficients[3, 1])
beta2_se <- exp(summary(log_damage_model)$coefficients[3, 2])

# Visualize
tibble(beta2 = seq(-(beta2_estimate + beta2_se),
                   beta2_estimate + beta2_se,
                   length.out = 200),
       density = dnorm(beta2, mean = 0, sd = beta2_se)) %>% 
  ggplot(aes(beta2, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta2_estimate, color = "firebrick") +
  labs(x = "Beta 2",
       y = "Density") +
  theme_minimal()

ggsave("log_beta2_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta1
pval_beta2 <- 2 * pnorm(-abs(beta2_estimate), mean = 0, sd = beta2_se)
print(paste0("The p-value for beta 2 is: ", pval_beta2, "."))
```

### 3. Time
```{r}
#| code-summary: Expand Code
# Distribution of Beta3
beta3_estimate <- exp(summary(log_damage_model)$coefficients[4, 1])
beta3_se <- exp(summary(log_damage_model)$coefficients[4, 2])

# Visualize
tibble(beta3 = seq(-(beta3_estimate + beta3_se),
                   beta3_estimate + beta3_se,
                   length.out = 200),
       density = dnorm(beta3, mean = 0, sd = beta3_se)) %>% 
  ggplot(aes(beta3, density)) +
  geom_line(color = "cornflowerblue") +
  geom_vline(xintercept = beta3_estimate, color = "firebrick") +
  labs(x = "Beta 3",
       y = "Density") +
  theme_minimal()

ggsave("log_beta3_density.jpg")
```

Calculate the probability of the point estimate under the null:
```{r}
#| code-summary: Expand Code
# p-value for Beta3
pval_beta3 <- 2 * pnorm(-abs(beta3_estimate), mean = 0, sd = beta3_se)
print(paste0("The p-value for beta 3 is: ", pval_beta3, "."))
```

## Conclusions

I found that of my original considerations for predictor variables, `max wind speed`, `rainfall`, and `time` were the best predictors for `damage costs`. The `number of places affected` did not prove to be beneficial to add to the model.

I found that we could not reject the null hypothesis for either model. Thus, we could not rule out that the effect of `time` on `damage costs` could be due to random chance. 

A few factors are not considered in these models:

- cost of inflation

Additional predictor variables that would be of interest[^4]:

- storm surge
- water temperature

[^4]: [NASA article](https://science.nasa.gov/earth/climate-change/a-force-of-nature-hurricanes-in-a-changing-climate/)

## Next steps

Given more time, I would have been interested in compiling a better dataset. The dataset used in this analysis isn't the best (no original source or thorough descriptions of variables i.e. rainfall?).
